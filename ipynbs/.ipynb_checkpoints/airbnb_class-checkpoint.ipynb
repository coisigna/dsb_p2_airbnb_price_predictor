{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633b15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import folium\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pickle\n",
    "\n",
    "# import pys/airbnb_class.py as ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67785752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class airbnb:\n",
    "    \n",
    "    def __init__(self, data, city_names = None, file = \"csv\"):\n",
    "                    \n",
    "        if (file == \"csv\") and (city_names is not None):\n",
    "            \n",
    "            self.l_dfs = list()\n",
    "            \n",
    "            for enum, dataset in enumerate(data):\n",
    "                \n",
    "                self.l_dfs.append(pd.read_csv(dataset))\n",
    "                \n",
    "                self.l_dfs[enum].drop(\"source\", axis = 1, inplace = True)\n",
    "                \n",
    "                self.l_dfs[enum][\"city\"] = city_names[enum].lower()\n",
    "        \n",
    "            self.df = pd.concat(self.l_dfs)\n",
    "            \n",
    "            print(\"Instance created!\")\n",
    "            \n",
    "        elif file == \"dataframe\":\n",
    "            \n",
    "            self.l_dfs = list()\n",
    "\n",
    "            for enum, dataframe in enumerate(data):\n",
    "                \n",
    "                self.l_dfs.append(dataframe)\n",
    "                                        \n",
    "            self.df = pd.concat(self.l_dfs)\n",
    "            \n",
    "            print(\"Instance created!\")\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(\"Only csv or dataframe are valid inputs, and city_names cannot be empty\")\n",
    "            \n",
    "    def return_initial_df(self):\n",
    "    \n",
    "        return self.df\n",
    "    \n",
    "    def display__initial_df(self):\n",
    "    \n",
    "        display(self.df)\n",
    "\n",
    "    def clean_tested_columns(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Sets predefined columns, transforms price to a float column and separates bathroom_text \n",
    "        into 3 different categories, private, shared and unknown.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sets predefined columns\n",
    "        \n",
    "        tested_cols = ['neighbourhood_cleansed', 'city',\n",
    "                       'room_type', 'accommodates', 'availability_365',\n",
    "                       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
    "                       'minimum_nights', 'maximum_nights',\n",
    "                       'number_of_reviews', 'reviews_per_month', 'host_total_listings_count']\n",
    "        \n",
    "        self.df_cleaned = self.df[tested_cols]\n",
    "        \n",
    "        # Transforms price to a float column\n",
    "        \n",
    "        self.df_cleaned[\"price\"] = self.df_cleaned[\"price\"].apply(lambda x: float(x.strip(\"$\").replace(',', '')) if pd.notnull(x) else x).values\n",
    "            \n",
    "        # Get numbers out of bathroom_text columns\n",
    "        \n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"bathrooms_text\"].isnull() == False]\n",
    "\n",
    "        l_nums = [re.findall(r'\\d+',i) for i in self.df_cleaned[\"bathrooms_text\"].values]\n",
    "\n",
    "        l_nums_completed = []\n",
    "\n",
    "        for i in l_nums:\n",
    "\n",
    "            if len(i) > 1:\n",
    "\n",
    "                l_nums_completed.append('.'.join(i))\n",
    "\n",
    "            elif len(i) == 0:\n",
    "\n",
    "                l_nums_completed.append('0')\n",
    "\n",
    "            else:\n",
    "\n",
    "                l_nums_completed.append(i[0])\n",
    "                \n",
    "        # Replace bathrooms_text with floats\n",
    "        \n",
    "        self.df_cleaned[\"bathrooms_text\"] = l_nums_completed\n",
    "\n",
    "        self.df_cleaned[\"bathrooms_text\"] = self.df_cleaned[\"bathrooms_text\"].astype(\"float64\")\n",
    "        \n",
    "        # Amenities\n",
    "                \n",
    "        l_amenities_cleaned = list()\n",
    "        \n",
    "        for i in self.df_cleaned[\"amenities\"]:\n",
    "\n",
    "            l_amenities_cleaned.append(json.loads(i))\n",
    "\n",
    "        # Most relevant amenities, detailed analysis in the EDA file\n",
    "\n",
    "        l_amenities_valuables = ['Long term stays allowed','Cooking basics','Dishes and silverware','Essentials','Coffee maker','Hair dryer','Microwave','Refrigerator','Heating','Air conditioning']\n",
    "\n",
    "        for j in l_amenities_valuables:\n",
    "\n",
    "            self.df_cleaned[j] = [1 if j in i else 0 for i in l_amenities_cleaned]\n",
    "\n",
    "        self.df_cleaned.drop(\"amenities\", axis =1, inplace=True)\n",
    "    \n",
    "        # Room type\n",
    "        \n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"room_type\"] != \"Hotel room\"]\n",
    "        self.df_cleaned = pd.concat([self.df_cleaned, pd.get_dummies(data = self.df_cleaned[\"room_type\"])], axis = 1).drop(\"room_type\", axis = 1)\n",
    "        \n",
    "        self.df_cleaned.dropna(inplace = True)\n",
    "        \n",
    "    def return_cleaned(self):\n",
    "        \n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def display_cleaned(self):\n",
    "        \n",
    "        display(self.df_cleaned)\n",
    "    \n",
    "    def remove_outliers(self, accommodates = 8, bathrooms_min = 1, bathrooms_max = 2, bedrooms = 4, beds_min = 1, beds_max = 5, minimum_nights = 30,\n",
    "                       maximum_nights = 70000, nreviews = 375, reviews_pmonth = 9, price = 350, htlc = 50000):\n",
    "\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"accommodates\"] <= accommodates]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"bathrooms_text\"].between(bathrooms_min, bathrooms_max)]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"bedrooms\"] <= bedrooms]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"beds\"].between(beds_min, beds_max)]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"minimum_nights\"] <= minimum_nights]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"maximum_nights\"] <= maximum_nights]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"number_of_reviews\"] <= nreviews]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"reviews_per_month\"] <= reviews_pmonth]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"price\"] <= price]\n",
    "        self.df_cleaned = self.df_cleaned[self.df_cleaned[\"host_total_listings_count\"] <= htlc]\n",
    "\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def display_outliers(self):\n",
    "        \n",
    "        for i in self.df_cleaned.columns:\n",
    "    \n",
    "            print(i)\n",
    "            sns.kdeplot(self.df_cleaned[i])\n",
    "            plt.show()\n",
    "\n",
    "    def label_encoding(self, df = None):\n",
    "        \n",
    "        if df is None:\n",
    "            df = self.df_cleaned\n",
    "            \n",
    "        city_encoder = LabelEncoder()\n",
    "        df[\"city\"] = city_encoder.fit_transform(df[\"city\"])\n",
    "        neighbourhood_encoder = LabelEncoder()\n",
    "        df[\"neighbourhood_cleansed\"] = neighbourhood_encoder.fit_transform(df[\"neighbourhood_cleansed\"])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def normalize(self, df = None):\n",
    "        \n",
    "        if df is None:\n",
    "            df = self.df_cleaned\n",
    "            \n",
    "        self.x_scaler = MinMaxScaler()\n",
    "        self.df_cleaned[self.df_cleaned.drop(\"price\", axis = 1).columns] = self.x_scaler.fit_transform(self.df_cleaned[self.df_cleaned.drop(\"price\", axis = 1).columns])\n",
    "\n",
    "        self.y_scaler = MinMaxScaler()\n",
    "        self.df_cleaned[\"price\"] = self.y_scaler.fit_transform(self.df_cleaned[[\"price\"]]).flatten()\n",
    "        \n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def tts(self):\n",
    "        \n",
    "        self.X = self.df_cleaned.drop([\"price\"], axis = 1)\n",
    "        self.y = self.df_cleaned[\"price\"]\n",
    "                \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "        print(f\"X_train: {self.X_train.shape} | y_train: {self.y_train.shape}\")\n",
    "        print(f\"X_test: {self.X_test.shape} | y_test: {self.y_test.shape}\")\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        models = [LinearRegression(), KNeighborsRegressor(), DecisionTreeRegressor(),\n",
    "                 RandomForestRegressor(), SVR(), AdaBoostRegressor(), GradientBoostingRegressor()]\n",
    "                \n",
    "        metrics = list()\n",
    "        \n",
    "        for model in models:\n",
    "            \n",
    "            # fit\n",
    "            \n",
    "            model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            # predict\n",
    "            \n",
    "            self.yhat = model.predict(self.X_test)\n",
    "            \n",
    "            # metrics\n",
    "            \n",
    "            r2 = r2_score(self.y_test, self.yhat)\n",
    "            mse = mean_squared_error(self.y_test, self.yhat)\n",
    "        \n",
    "            metrics.append([str(model), r2, mse, model])\n",
    "            \n",
    "        self.df_metrics = pd.DataFrame(data = metrics, columns = [\"model_name\", \"r2\", \"mse\", \"model\"])\n",
    "        self.df_metrics.sort_values(by = \"r2\", ascending = False, inplace= True)\n",
    "        \n",
    "    def return_metrics(self):\n",
    "        \n",
    "        return self.df_metrics\n",
    "    \n",
    "    def display_metrics(self):\n",
    "        \n",
    "        display(self.df_metrics)\n",
    "        \n",
    "    def model_feature_importances(self, model):\n",
    "        \n",
    "        importances = np.argsort(model.feature_importances_)[::-1]\n",
    "        d_importances = dict()\n",
    "        \n",
    "        for i in importances:\n",
    "\n",
    "            d_importances[i] = [model.feature_importances_[i]*100, self.df_cleaned.drop(\"price\", axis = 1).columns[i]]\n",
    "            print(i, model.feature_importances_[i]*100, self.df_cleaned.drop(\"price\", axis = 1).columns[i])\n",
    "            \n",
    "        return d_importances\n",
    "    \n",
    "    def grid_search_cv_tuning(self):\n",
    "        \n",
    "        model = RandomForestRegressor()\n",
    "        \n",
    "        params = {\"n_estimators\" : [i for i in range(100, 200, 50)],\n",
    "                  \"max_depth\"    : [8],#, 10, 12, 14, 16],\n",
    "                  \"max_features\" : [\"log2\", \"sqrt\"]}\n",
    "\n",
    "        scorers = {\"r2\", \"neg_mean_squared_error\"}\n",
    "\n",
    "        grid_solver = GridSearchCV(estimator  = model, \n",
    "                                   param_grid = params, \n",
    "                                   scoring    = scorers,\n",
    "                                   cv         = 2,\n",
    "                                   refit      = \"r2\",\n",
    "                                   n_jobs     = -1, \n",
    "                                   verbose    = 2)\n",
    "\n",
    "        self.model_result = grid_solver.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        d_validations = {\"Best Estimator\" : self.model_result.best_estimator_,\n",
    "                         \"Mean Test R**2\" : self.model_result.cv_results_[\"mean_test_r2\"].max(),\n",
    "                         \"Best Score\"     : self.model_result.best_score_}\n",
    "        \n",
    "        self.df_validations = pd.DataFrame(data    = d_validations.items(), \n",
    "                                           columns = [\"Validation\",\"Result\"])\n",
    "        \n",
    "    def return_model_result_gcv(self):\n",
    "        \n",
    "        return self.model_result\n",
    "        \n",
    "    def return_validations_gcv(self):\n",
    "        \n",
    "        return self.df_validations\n",
    "                                           \n",
    "    def return_validations_gcv(self):\n",
    "        \n",
    "        return self.df_validations\n",
    "    \n",
    "    def final_trial_model(self, max_depth = 16, max_features = 'sqrt', n_estimators = 800, random_state = 42):\n",
    "        \n",
    "        '''It trains the best model with the features recomended'''\n",
    "        \n",
    "        model = RandomForestRegressor(max_depth, max_features, n_estimators, random_state)\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        self.yhat = model.predict(self.X_test)\n",
    "    \n",
    "        return f\"r**2 = {r2_score(self.y_test, self.yhat)}\"\n",
    "    \n",
    "    def train_final_model(self, max_depth, max_features, n_estimators,random_state):\n",
    "        \n",
    "        '''Returns the definitive model'''\n",
    "        \n",
    "        self.X_def = self.df_cleaned.drop([\"price\"], axis = 1)\n",
    "        self.y_def = self.df_cleaned[\"price\"]\n",
    "        \n",
    "        model = RandomForestRegressor(max_depth = max_depth, max_features = max_features, n_estimators = n_estimators, random_state = random_state)\n",
    "        model.fit(self.X_def, self.y_def)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict(self, array, model):  \n",
    "        \n",
    "        '''Predicts the price given a cleaned array with te features needed'''\n",
    "        \n",
    "        self.price_predicted = self.y_scaler.inverse_transform([model.predict([array])])\n",
    "    \n",
    "    def return_prediction(self):\n",
    "        \n",
    "        return self.price_predicted\n",
    "    \n",
    "    def save_model(self, name, ext, model):\n",
    "    \n",
    "        with open(f\"{name}.{ext}\", \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "            \n",
    "    def load_model(self, name, ext):\n",
    "        \n",
    "        with open(f\"{name}.{ext}\", \"rb\") as file:\n",
    "            self.model = pickle.load(file)\n",
    "            \n",
    "        return self.model\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cbde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets used\n",
    "madrid = \"datasets/madrid.csv\"\n",
    "barcelona = \"datasets/barcelona.csv\"\n",
    "london = \"datasets/london.csv\"\n",
    "\n",
    "d_csvs, d_names = dict(), dict()\n",
    "\n",
    "d_csvs[\"csvs1\"] = [madrid, barcelona]\n",
    "d_csvs[\"csvs2\"] = [london]\n",
    "\n",
    "d_names[\"names1\"] = [\"madrid\",\"barcelona\"]\n",
    "d_names[\"names2\"] = [\"london\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42370d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance created!\n",
      "Instance created!\n",
      "CPU times: user 2.68 s, sys: 235 ms, total: 2.92 s\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "d_dfs = dict()\n",
    "\n",
    "for i in range(1,3):\n",
    "    \n",
    "    d_dfs[f\"instance{i}\"] = airbnb(d_csvs[f\"csvs{i}\"],d_names[f\"names{i}\"], \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5502b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = d_dfs[\"instance1\"].return_initial_df()\n",
    "df_2 = d_dfs[\"instance2\"].return_initial_df()\n",
    "\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec70a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mix = airbnb(data= [df_1,df_2], file=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c89951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mix.return_initial_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instance.return_initial_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901505ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/5nbnv2696vlgb605wf3g6wwh0000gn/T/ipykernel_2277/1270400121.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df_cleaned[\"price\"] = self.df_cleaned[\"price\"].apply(lambda x: float(x.strip(\"$\").replace(',', '')) if pd.notnull(x) else x).values\n",
      "/var/folders/10/5nbnv2696vlgb605wf3g6wwh0000gn/T/ipykernel_2277/1270400121.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df_cleaned[\"price\"] = self.df_cleaned[\"price\"].apply(lambda x: float(x.strip(\"$\").replace(',', '')) if pd.notnull(x) else x).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 82.6 ms, total: 1.41 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for instance in d_dfs.values():\n",
    "    \n",
    "    instance.clean_tested_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7052a595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "l_dfs = list()\n",
    "\n",
    "for instance in d_dfs.values():\n",
    "    \n",
    "    l_dfs.append(instance.return_cleaned())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09199853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.1 ms, sys: 2.29 ms, total: 44.4 ms\n",
      "Wall time: 44.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for instance in d_dfs.values():\n",
    "    \n",
    "    instance.remove_outliers()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca68b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.8 ms, sys: 2.19 ms, total: 43 ms\n",
      "Wall time: 42.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for instance in d_dfs.values():\n",
    "    instance.label_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3a2872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66.6 ms, sys: 11.4 ms, total: 77.9 ms\n",
      "Wall time: 76.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for instance in d_dfs.values():\n",
    "    instance.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7c622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (16786, 25) | y_train: (16786,)\n",
      "X_test: (4197, 25) | y_test: (4197,)\n",
      "X_train: (33600, 25) | y_train: (33600,)\n",
      "X_test: (8400, 25) | y_test: (8400,)\n",
      "CPU times: user 13.5 ms, sys: 3.23 ms, total: 16.7 ms\n",
      "Wall time: 15.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for instance in d_dfs.values():\n",
    "    instance.tts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9700a4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "CPU times: user 2.75 s, sys: 220 ms, total: 2.97 s\n",
      "Wall time: 9.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for instance in d_dfs.values():\n",
    "    instance.grid_search_cv_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c706570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_models_resulted = dict()\n",
    "\n",
    "for name,instance in d_dfs.items():\n",
    "    d_models_resulted[name] = instance.return_model_result_gcv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d_dfs[\"instance2\"].return_validations_gcv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2903f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Result\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_models_resulted[\"instance1\"].cv_results_[\"mean_test_r2\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_models_resulted[\"instance1\"].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_models_resulted[\"instance1\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c66959",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_validations_resulted = dict()\n",
    "\n",
    "for name, instance in d_dfs.items():\n",
    "    d_validations_resulted[name] = instance.return_validations_gcv()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bed807",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_validations_resulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2eca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dfs[\"instance1\"].save_model(\"model_1\", \"pkl\",d_models_resulted[\"instance1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_models_resulted[\"instance1\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_validations = {\"best\":\"patata\", \"middle\":\"zanahoria\",\"top\":\"pimiento\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = l_validations.items(), columns = [\"Score\",\"Ingredient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e8157df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = d_dfs[\"instance1\"].train_final_model(8, 'sqrt', 800, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93e9981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...max_depth=8, max_features=sqrt, n_estimators=150; total time=   1.1s\n",
      "[CV] END ...max_depth=8, max_features=log2, n_estimators=100; total time=   1.1s\n",
      "[CV] END ...max_depth=8, max_features=log2, n_estimators=100; total time=   0.7s\n",
      "[CV] END ...max_depth=8, max_features=log2, n_estimators=100; total time=   1.1s\n",
      "[CV] END ...max_depth=8, max_features=log2, n_estimators=100; total time=   0.7s\n",
      "[CV] END ...max_depth=8, max_features=sqrt, n_estimators=100; total time=   1.3s\n",
      "[CV] END ...max_depth=8, max_features=sqrt, n_estimators=100; total time=   0.8s\n",
      "[CV] END ...max_depth=8, max_features=sqrt, n_estimators=100; total time=   1.3s\n",
      "[CV] END ...max_depth=8, max_features=sqrt, n_estimators=100; total time=   0.8s\n",
      "[CV] END ...max_depth=8, max_features=log2, n_estimators=150; total time=   1.6s\n",
      "[CV] END ...max_depth=8, max_features=log2, n_estimators=150; total time=   1.0s\n",
      "[CV] END ...max_depth=8, max_features=log2, n_estimators=150; total time=   1.6s\n",
      "[CV] END ...max_depth=8, max_features=log2, n_estimators=150; total time=   1.0s\n",
      "[CV] END ...max_depth=8, max_features=sqrt, n_estimators=150; total time=   1.7s\n",
      "[CV] END ...max_depth=8, max_features=sqrt, n_estimators=150; total time=   1.1s\n",
      "[CV] END ...max_depth=8, max_features=sqrt, n_estimators=150; total time=   1.7s\n"
     ]
    }
   ],
   "source": [
    "d_dfs[\"instance1\"].save_model(name = \"model1\", ext = 'sav', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865eae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
